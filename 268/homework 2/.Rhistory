plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 500 trees")
# BART with 10 trees
post.burn.in <- 10000
burn.in <- 1000
num.tree <- 10
power.par.tree.prior <- 2
base.par.tree.prior <- 0.95
RT.BART <- bart(X.train, y.train, X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in,printevery=100, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=TRUE)
par(mfrow=c(2,2))
n = 200
p = 200
load(sprintf("dat2i_n%s_p%s.Rda",n,p))
# RF with 10 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=10)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 10 trees")
# RF with 500 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=500)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 500 trees")
# BART with 10 trees
post.burn.in <- 10000
burn.in <- 1000
num.tree <- 10
power.par.tree.prior <- 2
base.par.tree.prior <- 0.95
RT.BART <- bart(dat$X, dat$y, dat$X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in,printevery=100, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=TRUE)
ytest.BART <- RT.BART$yhat.test ##posterior predictive samples at test covaraites
quant.BART <- apply(ytest.BART,2,quantile,c(.025,.975))
length <- mean(quant.BART[2,]-quant.BART[1,])
coverage <- length(intersect(which(y.test>quant.BART[1,]),which(y.test<quant.BART[2,])))/100
RT.BART <- bart(dat$X, dat$y, dat$X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=FALSE)
par(mfrow=c(2,2))
n = 200
p = 200
load(sprintf("dat2i_n%s_p%s.Rda",n,p))
# RF with 10 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=10)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 10 trees")
# RF with 500 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=500)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 500 trees")
# BART with 10 trees
post.burn.in <- 10000
burn.in <- 1000
num.tree <- 10
power.par.tree.prior <- 2
base.par.tree.prior <- 0.95
RT.BART <- bart(dat$X, dat$y, dat$X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=FALSE)
ytest.BART <- RT.BART$yhat.test ##posterior predictive samples at test covaraites
quant.BART <- apply(ytest.BART,2,quantile,c(.025,.975))
length <- mean(quant.BART[2,]-quant.BART[1,])
coverage <- length(intersect(which(dat$y.test>quant.BART[1,]),which(dat$y.test<quant.BART[2,])))/100
MSPE.out <- mean((y.test-colMeans(ytest.BART))^2)
par(mfrow=c(2,2))
n = 200
p = 200
load(sprintf("dat2i_n%s_p%s.Rda",n,p))
# RF with 10 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=10)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 10 trees")
# RF with 500 trees
y = dat$y
X = dat$X
df = data.frame(y,X)
names(df) <- make.names(names(df))
CRForest <- randomForest(dat$y~.,data=df,mtry=max(floor(ncol(dat$X)/3),1),ntree=500)
Cpredicted.test <- predict(CRForest,data.frame(dat$X.test),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
Cpredicted.train <- predict(CRForest,data.frame(dat$X),type="response",norm.votes=TRUE,predict.all=TRUE, proximity=FALSE,nodes=FALSE)
MSPE.out = mean((as.numeric(Cpredicted.test$aggregate)-dat$y.test)^2)
#MSPE.in = mean((as.numeric(Cpredicted.train$aggregate)-dat$y)^2)
ub.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.975))
lb.y = apply(Cpredicted.test$individual,1,function(x) quantile(x,0.025))
p.mean.y = as.numeric(Cpredicted.test$aggregate)
length = mean(ub.y-lb.y)
coverage = length(intersect(which(dat$y.test>lb.y),which(dat$y.test<ub.y)))/100
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("Random Forest, 500 trees")
# BART with 10 trees
post.burn.in <- 10000
burn.in <- 1000
num.tree <- 10
power.par.tree.prior <- 2
base.par.tree.prior <- 0.95
RT.BART <- bart(dat$X, dat$y, dat$X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=FALSE)
ytest.BART <- RT.BART$yhat.test ##posterior predictive samples at test covaraites
quant.BART <- apply(ytest.BART,2,quantile,c(.025,.975))
length <- mean(quant.BART[2,]-quant.BART[1,])
coverage <- length(intersect(which(dat$y.test>quant.BART[1,]),which(dat$y.test<quant.BART[2,])))/100
MSPE.out <- mean((dat$y.test-colMeans(ytest.BART))^2)
ub.y = quant.BART[2,]
lb.y = quant.BART[1,]
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("in MSPE=%s",round(MSPE.in,3)),sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("BART, 10 trees")
# BART with 500 trees
post.burn.in <- 10000
burn.in <- 1000
num.tree <- 500
power.par.tree.prior <- 2
base.par.tree.prior <- 0.95
RT.BART <- bart(dat$X, dat$y, dat$X.test,sigest=2, sigdf=3, sigquant=.90,
k=2.0,power=power.par.tree.prior,base=base.par.tree.prior,
binaryOffset=0,ntree=num.tree,
ndpost=post.burn.in, nskip=burn.in, keepevery=1,
keeptrainfits=TRUE,usequants=FALSE, numcut=100, printcutoffs=0,
verbose=FALSE)
ytest.BART <- RT.BART$yhat.test ##posterior predictive samples at test covaraites
quant.BART <- apply(ytest.BART,2,quantile,c(.025,.975))
length <- mean(quant.BART[2,]-quant.BART[1,])
coverage <- length(intersect(which(dat$y.test>quant.BART[1,]),which(dat$y.test<quant.BART[2,])))/100
MSPE.out <- mean((dat$y.test-colMeans(ytest.BART))^2)
ub.y = quant.BART[2,]
lb.y = quant.BART[1,]
par(mar=c(4,4,1,1))
plot(dat$y.test,col="red",pch=19,xlab="response index",ylab="new responses, y",cex.axis=1,cex.lab=1,xlim=c(-0.2,100.2),ylim=c(-50,300))
arrows(1:100, lb.y, 1:100,  ub.y, length=0.01, angle=90, code=3, lwd=2, col="grey")
points(1:100, p.mean.y, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:50, rev(1:50)), c(ub.y, rev(lb.y) ),col="grey", border = NA)
#lines(p.mean.y,lwd=0.6)
legend("topright",legend = c(sprintf("in MSPE=%s",round(MSPE.in,3)),sprintf("out MSPE=%s",round(MSPE.out,3)),sprintf("length=%s",round(length,3)),sprintf("coverage=%s",round(coverage,3))),cex=0.8,inset=0.04,box.lty=0)
points(dat$y.test,col="red",pch=19,lwd=1,cex=.5)
title("BART, 500 trees")
dim(ytest.BART)
library(mnormt)
library(randomForest)
library(BayesTree)
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R')
#sample size
nn = 1:2
all.n = c(50,200)
#beta size
np = 1
all.p = c(20)
#beta covariance
nS = 2
S2 = function(p){Sig = diag(p)
for(r in 1:p){
for(c in 1:p){
Sig[r,c] = (0.6)^(abs(r-c))}}
return(Sig)}
#beta values
nB = 1
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
include.zero.indicator
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
library(mnormt)
library(randomForest)
library(BayesTree)
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R')
#sample size
nn = 1:2
all.n = c(50,200)
#beta size
np = 1
all.p = c(20)
#beta covariance
nS = 2
S2 = function(p){Sig = diag(p)
for(r in 1:p){
for(c in 1:p){
Sig[r,c] = (0.6)^(abs(r-c))}}
return(Sig)}
#beta values
nB = 1
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
help(rgamma)
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/gprior_regression.R', echo=TRUE)
par(mfrow=c(2,2))
# load datasets
for(i in nn){
n = all.n[i]
p = 20
setwd("~/Google Drive/year 3/268/homework 2")
load(sprintf("dat_n%s_p20_S2_B1.Rda",n))
gp = gprior_regression(dat$y,dat$X)
#save needed values for next parts E(beta|y), MSE
ub.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.975))
lb.beta = apply(gp$ps.beta,1,function(x) quantile(x,0.025))
p.mean.beta = rowMeans(gp$ps.beta)
include.zero.indicator = as.numeric(lb.beta <= 0)*as.numeric(ub.beta >= 0)
interval.cols = rep("grey",p)
interval.cols[which(include.zero.indicator==1)] = "blue"
MSE = mean( (dat$B-p.mean.beta)^2 )
par(mar=c(4,4,1,1))
plot(dat$B,col="red",pch=19,xlab="predictor index",ylab="coefficents",cex.axis=1,cex.lab=1,ylim=c(-0.5,3.5),xlim=c(-0.2,20.2))
arrows(1:p, lb.beta, 1:p,  ub.beta, length=0.01, angle=90, code=3, lwd=2, col=interval.cols)
points(1:p, p.mean.beta, pch=19, lwd=1, col="black", cex=.5)
#polygon(c(1:p, rev(1:p)), c(ub.beta, rev(lb.beta) ),col="grey", border = NA)
points(dat$B,col="red",pch=19,lwd=1,cex=.5)
legend("topright",legend = c(sprintf("n=%s",n),sprintf("MSE=%s",round(MSE,3))),cex=0.8,inset=0.04,box.lty=0)
}
source('~/Google Drive/year 3/268/homework 2/hw2_datageneration.R', echo=TRUE)
